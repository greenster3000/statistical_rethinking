---
title: "Statistical Rethinking 2 - Chapter 14 Notes"
output: html_notebook
---
```{r warning=FALSE}
library(MASS)
library(ellipse)
library(rethinking)
options(mc.cores = parallel::detectCores())
rstan_options(auto_write = TRUE)
```

Three things: 

1. Varying slopes and their covariance with varying intercepts.

2. Variation of slopes providing hints of missing variables.

3. The first examples are only where our pooling occurs across different 
categories that are all equally different. E.g. pools areall different pools. 
Can extend the above to instances with categories such as age or location that
are continuous.

## 14.1 Simulating varying slopes covarying with varying intercepts

As well as pooling intercepts as we did with the ponds and frogs, we can pool 
slopes as well. 

Even better, we can squeeze more information out by pooling slopes & intercepts
together. 

Need a covariance matrix to do this. Instead of two independent gaussian dists,
each with their own slope and intercept

$a \sim \mathcal{N}(\mu_a, \sigma_a)$
 
$b \sim \mathcal{N}(\mu_b, \sigma_b)$  


Instead we need a multivariate Gaussian dist with two dimensions. One for slopes
, one for intercepts, and a 2-D covariance matrix

### 14.1 Simulate Population

This is the simulation and plotting of the actual cafes and their underlying
morning wait times and underlying differences to wait times in the afternoon. 
These are the **actual** means of all the cafes we're going to be simulating
*from*.
```{r}
a <- 3.5        # average morning wait time aka intercept
b <- (-1)       # average difference to afternoon wait time aka slope
sigma_a <- 1    # std dev in morning wait time aka intercepts
sigma_b <- 0.5  # std dev in difference to afternoon wait time aka slopes
rho <- (-0.7)   # correlation between intercepts and slopes. 
# ie cafes with a bigger morning wait time have larger difference to their affy

Mu <- c(a, b)   # Mu of the MV-norm
cov_ab <- sigma_a * sigma_b * rho # covariance between slopes and intercepts
Sigma <- matrix(c(sigma_a^2, cov_ab, cov_ab, sigma_b^2), ncol = 2) # covar matx

N_cafes <- 20

set.seed(5)
# Simulate morning wait times and difference to afternoon wait times for N_Cafes
vary_effects <- mvrnorm(N_cafes, Mu, Sigma)
a_cafe <- vary_effects[,1]
b_cafe <- vary_effects[,2]
plot( a_cafe , b_cafe , col=rangi2 , 
      xlab="intercepts (a_cafe)",
      ylab="slopes (b_cafe)"
)

for (l in c (0.1, 0.3, 0.5, 0.8, 0.99) )
  lines(ellipse(Sigma, centre=Mu, level=l), 
        col=col.alpha("black", 0.2))
```
Can now see a sample of 20 cafes and their mean morning wait times (a_cafe) and 
their mean difference of waiting time in the affy (b_cafe) 
Covariance is helped to be shown by the ellipses. These indivudual means are 
what we're trying to get back from our model later on. We can also see how well
we did in guessing the overall population alpha and beta here.

### 14.1.2 Simulate Observations from this population

Now going to have our robots "collect data" from these 20 cafes by visiting each
five times in the morning and five in the afternoon. 
```{r}
set.seed(22)
N_visits <- 10
afternoon <- rep(0:1, N_visits*N_cafes/2) # repeats 100 times 0 1
cafe_id <- rep(1:N_cafes, each=N_visits)  # repeats 10 times each of 1-20

# Takes our a_cafe and b_cafe from each of the 20 acrual simulated cafes above
mu <- a_cafe[cafe_id] + b_cafe[cafe_id] * afternoon

# Something we're defining ourselves
sigma <- 0.5

# Simulate an *individual* wait time for this particular cafe_id and if affy
wait <- rnorm(N_visits*N_cafes, mu, sigma)

# Finally make a dataframe of our 20 cafes * (5 morning + 5 afternoon) visits
d <- data.frame(cafe=cafe_id, afternoon=afternoon, wait=wait)
```

This data is balanced, each cafe is visited 5 times in the morning and 5 times 
in the afternoon. IRL, data isn't balanced, and pooling shines even more as it 
pools where it is needed most (under-sampled categories) and backs off where we 
need it least (over-sampled)

This is what goes in to the model, to try and get back our mean morning and 
afternoon wait times per cafe. 

As always, best to simulate first. Beware though, here we know the process used
to simulate our data, and can use the exact same structure to model that process
below. IRL, we won't be able to do this. The true data generating process is 
hidden, and our model will most likely be wrong or *mis-specified*. 

IRL, we can simulate data from a process, and see how different models, each of 
which has a *different* presumed data-generating process to the one we used to 
simulate the data. Then we can observe how they perform. 

In general, Bayesian inference doesn't depend on data-generating assumptions 
being true. Even our likelihood doesn't have to be true! Our likelihood is a 
prior for the data. 

### 14.1.3 Actually build a model

Reverse the proecss and try and recover our $\rho, \sigma_a, \sigma_b$ etc. from
our model.

Similar to before, but now have a multi-variate Gaussian, and the fancy
correlation matrix. Need a prior for this.

$W_i \sim \mathcal{N}(\mu_i, \sigma)$ 

Likelihood, we thinkg that all waiting 
times are sampled from a distribution. All we are prepared to say about this 
distribution is that it has a mean and sd. Each cafe has its own mean, sd is for
the population of cafes

$\mu_i = \alpha_{CAFE[i]} + \beta_{CAFE[i]}A_i$ 

Linear Model. Each cafe's mean 
wait time is determined by an intercept and a slope and whether it's the affy or
not.

$$
\begin{bmatrix}
\alpha_{CAFE} \\ \beta_{CAFE}
\end{bmatrix}
\sim MVNormal
\left(
\begin{bmatrix}
\alpha \\ \beta
\end{bmatrix},
\Sigma
\right)
$$
The fun part, same as the varying effects as before, but now in 2-d. This 
adaptively regularises the prior, but now also regularises the correlation 
between them as well. 

$$
\Sigma = 
\begin{pmatrix}
\sigma_\alpha & 0 \\ 0 & \sigma_\beta 
\end{pmatrix}
\mathbf{R}
\begin{pmatrix}
\sigma_\alpha & 0 \\ 0 & \sigma_\beta
\end{pmatrix}
$$
This is showing how we construct our covariance matrix.

$\alpha \sim \mathcal{N}(5,2)$ Prior for average intercept

$\beta \sim \mathcal{N}(-1,0.5)$ Prior for average slope

$\sigma \sim exp(1)$ Prior for std dev within all cafes

$\sigma_{\alpha} \sim exp(1)$ Prior for stddev among intercepts

$\sigma_{\beta} \sim exp(1)$ Prior for stddev among slope

$\mathbf{R} \sim LKJcorr(2)$ Prior for correlation matrix

Put this into an actual Stan Model
```{r echo=TRUE, warning=FALSE}
set.seed(867530)
m14.1 <- ulam(
  alist(
    wait ~ normal(mu, sigma),
    mu <- a_cafe[cafe] + b_cafe[cafe] * afternoon,
    c(a_cafe, b_cafe)[cafe] ~ multi_normal(c(a,b), Rho, sigma_cafe),
    a ~ normal(5, 2),
    b ~ normal(-1, 0.5),
    sigma_cafe ~ exponential(1),
    sigma ~ exponential(1),
    Rho ~ lkj_corr(2)
  ), data=d, chains=4
  
)
```

```{r}
precis(m14.1, depth=3)
```
Going to ignore these since Rhat looks to be one everywhere and we got a good
number of n_eff for everything
```{r warning=FALSE}
post <- extract.samples(m14.1)
dens(post$Rho[,1,2], xlim=c(-1,1)) # posterior
R <- rlkjcorr(1e4, K=2, eta = 2)   # prior
dens(R[,1,2], add=TRUE, lty=2)
```
Extract some samples from the posterior distribution, then plot the correlation
between intercepts and slopes. Dash is prior and solid is posterior. Doesn't
quite seem to refelct the -0.7 we put into the model. However, can see a
negative correlation between the two.

```{r}
mean(post$Rho[,1,2])

getmode <- function(v) {
   uniqv <- unique(v)
   uniqv[which.max(tabulate(match(v, uniqv)))]
}

getmode(round(post$Rho[,1,2], 2))
```

Text suggests running the same with flat priot of LKJcorr(1) and strongly 
regluarizing prior of LKJcorr(5) and see what happens. Let's try!

```{r warning=FALSE, include=FALSE}
set.seed(867530)
m14.1_flat <- ulam(
  alist(
    wait ~ normal(mu, sigma),
    mu <- a_cafe[cafe] + b_cafe[cafe] * afternoon,
    c(a_cafe, b_cafe)[cafe] ~ multi_normal(c(a,b), Rho, sigma_cafe),
    a ~ normal(5, 2),
    b ~ normal(-1, 0.5),
    sigma_cafe ~ exponential(1),
    sigma ~ exponential(1),
    Rho ~ lkj_corr(1)
  ), data=d, chains=4
)
m14.1_reg <- ulam(
  alist(
    wait ~ normal(mu, sigma),
    mu <- a_cafe[cafe] + b_cafe[cafe] * afternoon,
    c(a_cafe, b_cafe)[cafe] ~ multi_normal(c(a,b), Rho, sigma_cafe),
    a ~ normal(5, 2),
    b ~ normal(-1, 0.5),
    sigma_cafe ~ exponential(1),
    sigma ~ exponential(1),
    Rho ~ lkj_corr(5)
  ), data=d, chains=4
)
```
```{r}
post_flat <- extract.samples(m14.1_flat)
post_reg <- extract.samples(m14.1_reg)
dens(post_flat$Rho[,1,2], xlim=c(-1,1), lty=2) # posterior with eta = 1
dens(post$Rho[,1,2], add=TRUE, lty=3) # posterior with eta = 2
dens(post_reg$Rho[,1,2], add=TRUE, lty=4) # posterior with eta = 5
R <- rlkjcorr(1e4, K=2, eta = 2)   # prior
dens(R[,1,2], add=TRUE)
legend(x=0.5, y = 1.5, legend=c("Flat", "eta = 2", "Regularising", "Prior"), lty=c(2,3,4,1))
```
Can see the effect of increasing the $\eta$ we regularise more and hence pooling
has less of an effect, as expected.

Now, looking at shrinkage with the helps of plots.

#### Direct computation of the unpooled estimates from the simulated wait times
```{r}
a1 <- sapply( 1:N_cafes, 
              function(i) mean(wait[cafe_id==i & afternoon==0]))
b1 <- sapply( 1:N_cafes,
              function(i) mean(wait[cafe_id==i & afternoon==1])) - a1
# Note the (-a1) here to calcualte the SLOPE
```
#### Extracting the posterior means of the partially pooled estimates
Posterior estimates of the intercepts (a2) and slopes (b2).
```{r}
post <- extract.samples(m14.1)
a2 <- apply(post$a_cafe, 2, mean)
b2 <- apply(post$b_cafe, 2, mean)
```
#### Plot these with lines between them

```{r warning=FALSE}
plot(a1, b1, xlab="intercept", ylab="slope",
     pch=16, col=rangi2, ylim=c(min(b1) - 0.1, max(b1) + 0.1),
     xlim=c(min(a1) - 0.1, max(a1) + 0.1))
points(a2, b2, pch=1)
for ( i in 1:N_cafes ) lines ( c(a1[i], a2[i]), c(b1[i], b2[i]))
# Mu vector estimate is simply means of a_cafe and b_cafe
Mu_est <- c(mean(post$a), mean(post$b))
# Taking the corner element of the LKJcorr posterior
rho_est <- mean(post$Rho[,1,2])
sa_est <- mean(post$sigma_cafe[,1])
sb_est <- mean(post$sigma_cafe[,2])
cov_ab <- sa_est * sb_est * rho_est
Sigma_est <- matrix(c(sa_est^2, cov_ab, cov_ab, sb_est^2), ncol=2)
# Need all of these to plot the ellipses
for ( l in c(0.1, 0.3, 0.5, 0.8, 0.99))
  lines(ellipse(Sigma_est, centre=Mu_est, level=l),
        col=col.alpha("black", 0.2))
```
Blue filled points are unpooled estimates for each cafe, hollow are pooled 
estimates. As expected, cafes farther from the centre are shrunk more. Notice 
the effects of covariance here. The blue point with slope ~ 0 and intercept ~ 3
has its slope shrunk, as we would expect, given its estimated slope here is the 
second largest amongst all slopes. Cafes with slope ~ -1 lower do not have their
slope shrunk. However, the cafe at the top *also* has its intercept shrunk more
than those at (3, -1), due to the covariance. We are suspicious of a cafe 
retaining such an intercept if we reduce its slope. Angled lines reflect the 
correlation. 

#### Plotting on the outcome scale
Convert the slopes and intercepts to actual waiting times
```{r warning=FALSE}
wait_morning_direct <- a1
wait_afternoon_direct <- a1 + b1
wait_morning_pooled <- a2
wait_afternoon_pooled <- a2 + b2

plot(wait_morning_direct, wait_afternoon_direct, xlab="morning wait",
     ylab="afternoon wait", pch=16, col=rangi2,
     xlim=c(min(wait_morning_direct) - 0.1, max(wait_morning_direct) + 0.1),
     ylim=c(min(wait_afternoon_direct) - 0.1, max(wait_afternoon_direct) + 0.1)
)
points(wait_morning_pooled, wait_afternoon_pooled, pch=1)
for ( i in 1:N_cafes )
  lines(c(wait_morning_pooled[i], wait_morning_direct[i]), 
        c(wait_afternoon_pooled[i], wait_afternoon_direct[i]))
abline(a=0, b=1, lty=2)

# Now tricky-fold. Find shrinkage implied by the estimation on outcome the scale

# Generates 10k rows of morning waits (intercepts) and affy waits (slopes)
v <- mvrnorm(1e4, Mu_est, Sigma_est) 
# Store afternoon wait in second column 
v[,2] <- v[,1] + v[,2]
# Stats package functionto calculate the covariance of morn and affy wait times
Sigma_est_outcome <- cov(v)
# Need to create a copy to preserve the original in the parameter space
Mu_est_outcome <- Mu_est
Mu_est_outcome[2] <- Mu_est[1] + Mu_est[2]
# And use these two vectors of the MV-normal to draw the ellipses
for ( l in c(0.1, 0.3, 0.5, 0.8, 0.99))
  lines(ellipse(Sigma_est_outcome, centre=Mu_est_outcome, level=l),
        col=col.alpha("black", 0.5))
```
Again, can see that the we shrink more further away from the centre, that wait
times in the morning and afternoon are *positively* correlated in the same cafe.
However, all these cafes in the population lie below the dashe line. We wait 
longer in the aftrenoon.
